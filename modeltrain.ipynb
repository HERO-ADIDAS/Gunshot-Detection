{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 128, None,   0           []                               \n",
      "                                2)]                                                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 128, None, 6  1216        ['input_1[0][0]']                \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 64, None, 64  0           ['conv2d[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 64, None, 12  73856       ['max_pooling2d[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 32, None, 12  0          ['conv2d_1[0][0]']               \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, None, 25  295168      ['max_pooling2d_1[0][0]']        \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 16, None, 25  0          ['conv2d_2[0][0]']               \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, None, 256)    0           ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, None, 256)   263168      ['reshape[0][0]',                \n",
      " dAttention)                                                      'reshape[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, None, 256)    0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, None, 256)   0           ['dropout[0][0]',                \n",
      " da)                                                              'reshape[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, None, 256)   512         ['tf.__operators__.add[0][0]']   \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 128)    32896       ['layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, None, 256)    33024       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, None, 256)    0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, None, 256)   0           ['dropout_1[0][0]',              \n",
      " mbda)                                                            'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, None, 256)   512         ['tf.__operators__.add_1[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 256)         0           ['layer_normalization_1[0][0]']  \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " cate_fc (Dense)                (None, 4)            1028        ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dist_fc (Dense)                (None, 7)            1799        ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dire_fc (Dense)                (None, 6)            1542        ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 704,721\n",
      "Trainable params: 704,721\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "33/33 [==============================] - 5s 41ms/step - loss: 4.8707 - cate_fc_loss: 1.3777 - dist_fc_loss: 1.7158 - dire_fc_loss: 1.7773 - cate_fc_accuracy: 0.4937 - dist_fc_accuracy: 0.3195 - dire_fc_accuracy: 0.2589\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 3.6720 - cate_fc_loss: 0.8627 - dist_fc_loss: 1.3192 - dire_fc_loss: 1.4902 - cate_fc_accuracy: 0.6073 - dist_fc_accuracy: 0.4321 - dire_fc_accuracy: 0.3186\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 3.2888 - cate_fc_loss: 0.7826 - dist_fc_loss: 1.2100 - dire_fc_loss: 1.2962 - cate_fc_accuracy: 0.6343 - dist_fc_accuracy: 0.4976 - dire_fc_accuracy: 0.4398\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 2.9454 - cate_fc_loss: 0.7280 - dist_fc_loss: 1.0798 - dire_fc_loss: 1.1375 - cate_fc_accuracy: 0.6593 - dist_fc_accuracy: 0.5553 - dire_fc_accuracy: 0.4976\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 2.6419 - cate_fc_loss: 0.6277 - dist_fc_loss: 0.9710 - dire_fc_loss: 1.0432 - cate_fc_accuracy: 0.7084 - dist_fc_accuracy: 0.5890 - dire_fc_accuracy: 0.5313\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 2.3757 - cate_fc_loss: 0.5581 - dist_fc_loss: 0.8998 - dire_fc_loss: 0.9178 - cate_fc_accuracy: 0.7623 - dist_fc_accuracy: 0.6169 - dire_fc_accuracy: 0.5919\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 2.2008 - cate_fc_loss: 0.4922 - dist_fc_loss: 0.8245 - dire_fc_loss: 0.8841 - cate_fc_accuracy: 0.7719 - dist_fc_accuracy: 0.6420 - dire_fc_accuracy: 0.5938\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 2.7339 - cate_fc_loss: 0.6508 - dist_fc_loss: 1.0036 - dire_fc_loss: 1.0795 - cate_fc_accuracy: 0.6968 - dist_fc_accuracy: 0.5987 - dire_fc_accuracy: 0.5332\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 2.5065 - cate_fc_loss: 0.6249 - dist_fc_loss: 0.8982 - dire_fc_loss: 0.9834 - cate_fc_accuracy: 0.7218 - dist_fc_accuracy: 0.6208 - dire_fc_accuracy: 0.5476\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 2.1939 - cate_fc_loss: 0.5400 - dist_fc_loss: 0.7529 - dire_fc_loss: 0.9010 - cate_fc_accuracy: 0.7546 - dist_fc_accuracy: 0.6824 - dire_fc_accuracy: 0.5804\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 1s 18ms/step - loss: 2.0646 - cate_fc_loss: 0.4521 - dist_fc_loss: 0.7406 - dire_fc_loss: 0.8720 - cate_fc_accuracy: 0.8094 - dist_fc_accuracy: 0.6833 - dire_fc_accuracy: 0.5987\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 1.8685 - cate_fc_loss: 0.3886 - dist_fc_loss: 0.6834 - dire_fc_loss: 0.7965 - cate_fc_accuracy: 0.8354 - dist_fc_accuracy: 0.7007 - dire_fc_accuracy: 0.6044\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 1s 18ms/step - loss: 1.7223 - cate_fc_loss: 0.3323 - dist_fc_loss: 0.6385 - dire_fc_loss: 0.7515 - cate_fc_accuracy: 0.8691 - dist_fc_accuracy: 0.7180 - dire_fc_accuracy: 0.6506\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 1.6805 - cate_fc_loss: 0.2964 - dist_fc_loss: 0.6469 - dire_fc_loss: 0.7372 - cate_fc_accuracy: 0.8884 - dist_fc_accuracy: 0.7007 - dire_fc_accuracy: 0.6785\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 1.5726 - cate_fc_loss: 0.2640 - dist_fc_loss: 0.5960 - dire_fc_loss: 0.7126 - cate_fc_accuracy: 0.9009 - dist_fc_accuracy: 0.7507 - dire_fc_accuracy: 0.6882\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 1.5316 - cate_fc_loss: 0.2629 - dist_fc_loss: 0.5597 - dire_fc_loss: 0.7089 - cate_fc_accuracy: 0.8922 - dist_fc_accuracy: 0.7661 - dire_fc_accuracy: 0.7074\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 1.4922 - cate_fc_loss: 0.2568 - dist_fc_loss: 0.5739 - dire_fc_loss: 0.6615 - cate_fc_accuracy: 0.8980 - dist_fc_accuracy: 0.7507 - dire_fc_accuracy: 0.7122\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 1.4787 - cate_fc_loss: 0.2511 - dist_fc_loss: 0.5370 - dire_fc_loss: 0.6906 - cate_fc_accuracy: 0.8970 - dist_fc_accuracy: 0.7796 - dire_fc_accuracy: 0.6939\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 1.3930 - cate_fc_loss: 0.2453 - dist_fc_loss: 0.5146 - dire_fc_loss: 0.6331 - cate_fc_accuracy: 0.9134 - dist_fc_accuracy: 0.7902 - dire_fc_accuracy: 0.7267\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 1.3187 - cate_fc_loss: 0.1914 - dist_fc_loss: 0.5025 - dire_fc_loss: 0.6247 - cate_fc_accuracy: 0.9336 - dist_fc_accuracy: 0.7834 - dire_fc_accuracy: 0.7421\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 1s 16ms/step - loss: 1.5884 - cate_fc_loss: 0.2641 - dist_fc_loss: 0.5865 - dire_fc_loss: 0.7378 - cate_fc_accuracy: 0.9009 - dist_fc_accuracy: 0.7709 - dire_fc_accuracy: 0.6872\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 1.2408 - cate_fc_loss: 0.1927 - dist_fc_loss: 0.4868 - dire_fc_loss: 0.5613 - cate_fc_accuracy: 0.9317 - dist_fc_accuracy: 0.8094 - dire_fc_accuracy: 0.7709\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 1.2170 - cate_fc_loss: 0.1807 - dist_fc_loss: 0.4684 - dire_fc_loss: 0.5679 - cate_fc_accuracy: 0.9355 - dist_fc_accuracy: 0.8056 - dire_fc_accuracy: 0.7526\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 1s 16ms/step - loss: 1.1212 - cate_fc_loss: 0.1791 - dist_fc_loss: 0.4075 - dire_fc_loss: 0.5347 - cate_fc_accuracy: 0.9365 - dist_fc_accuracy: 0.8518 - dire_fc_accuracy: 0.7825\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 1.0420 - cate_fc_loss: 0.1343 - dist_fc_loss: 0.4044 - dire_fc_loss: 0.5033 - cate_fc_accuracy: 0.9509 - dist_fc_accuracy: 0.8393 - dire_fc_accuracy: 0.7892\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 0.9845 - cate_fc_loss: 0.1431 - dist_fc_loss: 0.3921 - dire_fc_loss: 0.4494 - cate_fc_accuracy: 0.9451 - dist_fc_accuracy: 0.8518 - dire_fc_accuracy: 0.8210\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 0.8744 - cate_fc_loss: 0.1039 - dist_fc_loss: 0.3493 - dire_fc_loss: 0.4212 - cate_fc_accuracy: 0.9673 - dist_fc_accuracy: 0.8547 - dire_fc_accuracy: 0.8470\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 0.7893 - cate_fc_loss: 0.0914 - dist_fc_loss: 0.3096 - dire_fc_loss: 0.3884 - cate_fc_accuracy: 0.9682 - dist_fc_accuracy: 0.8730 - dire_fc_accuracy: 0.8460\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 0.7803 - cate_fc_loss: 0.0932 - dist_fc_loss: 0.3092 - dire_fc_loss: 0.3780 - cate_fc_accuracy: 0.9605 - dist_fc_accuracy: 0.8758 - dire_fc_accuracy: 0.8585\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 0.6993 - cate_fc_loss: 0.0882 - dist_fc_loss: 0.2660 - dire_fc_loss: 0.3451 - cate_fc_accuracy: 0.9663 - dist_fc_accuracy: 0.9057 - dire_fc_accuracy: 0.8691\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 0.6543 - cate_fc_loss: 0.0738 - dist_fc_loss: 0.2603 - dire_fc_loss: 0.3202 - cate_fc_accuracy: 0.9721 - dist_fc_accuracy: 0.8989 - dire_fc_accuracy: 0.8864\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 0.6564 - cate_fc_loss: 0.0731 - dist_fc_loss: 0.2478 - dire_fc_loss: 0.3355 - cate_fc_accuracy: 0.9692 - dist_fc_accuracy: 0.9105 - dire_fc_accuracy: 0.8710\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 0.6158 - cate_fc_loss: 0.0670 - dist_fc_loss: 0.2394 - dire_fc_loss: 0.3093 - cate_fc_accuracy: 0.9702 - dist_fc_accuracy: 0.9076 - dire_fc_accuracy: 0.8845\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 0.5533 - cate_fc_loss: 0.0657 - dist_fc_loss: 0.2182 - dire_fc_loss: 0.2694 - cate_fc_accuracy: 0.9750 - dist_fc_accuracy: 0.9259 - dire_fc_accuracy: 0.8999\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 0.5275 - cate_fc_loss: 0.0555 - dist_fc_loss: 0.2278 - dire_fc_loss: 0.2442 - cate_fc_accuracy: 0.9817 - dist_fc_accuracy: 0.9124 - dire_fc_accuracy: 0.9143\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 0.4723 - cate_fc_loss: 0.0454 - dist_fc_loss: 0.1937 - dire_fc_loss: 0.2332 - cate_fc_accuracy: 0.9865 - dist_fc_accuracy: 0.9297 - dire_fc_accuracy: 0.9192\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 0.4891 - cate_fc_loss: 0.0593 - dist_fc_loss: 0.1962 - dire_fc_loss: 0.2336 - cate_fc_accuracy: 0.9808 - dist_fc_accuracy: 0.9211 - dire_fc_accuracy: 0.9076\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 0.4367 - cate_fc_loss: 0.0631 - dist_fc_loss: 0.1720 - dire_fc_loss: 0.2016 - cate_fc_accuracy: 0.9769 - dist_fc_accuracy: 0.9403 - dire_fc_accuracy: 0.9182\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 0.4415 - cate_fc_loss: 0.0626 - dist_fc_loss: 0.1895 - dire_fc_loss: 0.1895 - cate_fc_accuracy: 0.9798 - dist_fc_accuracy: 0.9403 - dire_fc_accuracy: 0.9336\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 0.7929 - cate_fc_loss: 0.1657 - dist_fc_loss: 0.3096 - dire_fc_loss: 0.3176 - cate_fc_accuracy: 0.9451 - dist_fc_accuracy: 0.8778 - dire_fc_accuracy: 0.8778\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 0.5457 - cate_fc_loss: 0.0726 - dist_fc_loss: 0.2339 - dire_fc_loss: 0.2393 - cate_fc_accuracy: 0.9711 - dist_fc_accuracy: 0.9220 - dire_fc_accuracy: 0.9163\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 0.4809 - cate_fc_loss: 0.0721 - dist_fc_loss: 0.1770 - dire_fc_loss: 0.2318 - cate_fc_accuracy: 0.9721 - dist_fc_accuracy: 0.9374 - dire_fc_accuracy: 0.9115\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 0.4092 - cate_fc_loss: 0.0921 - dist_fc_loss: 0.1548 - dire_fc_loss: 0.1622 - cate_fc_accuracy: 0.9634 - dist_fc_accuracy: 0.9480 - dire_fc_accuracy: 0.9451\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 1s 16ms/step - loss: 0.4292 - cate_fc_loss: 0.0730 - dist_fc_loss: 0.1887 - dire_fc_loss: 0.1675 - cate_fc_accuracy: 0.9692 - dist_fc_accuracy: 0.9346 - dire_fc_accuracy: 0.9432\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 0.3642 - cate_fc_loss: 0.0514 - dist_fc_loss: 0.1634 - dire_fc_loss: 0.1494 - cate_fc_accuracy: 0.9817 - dist_fc_accuracy: 0.9384 - dire_fc_accuracy: 0.9509\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 0.3234 - cate_fc_loss: 0.0421 - dist_fc_loss: 0.1436 - dire_fc_loss: 0.1377 - cate_fc_accuracy: 0.9865 - dist_fc_accuracy: 0.9442 - dire_fc_accuracy: 0.9490\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 0.2420 - cate_fc_loss: 0.0353 - dist_fc_loss: 0.1044 - dire_fc_loss: 0.1023 - cate_fc_accuracy: 0.9875 - dist_fc_accuracy: 0.9615 - dire_fc_accuracy: 0.9702\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 0.2696 - cate_fc_loss: 0.0447 - dist_fc_loss: 0.1034 - dire_fc_loss: 0.1214 - cate_fc_accuracy: 0.9846 - dist_fc_accuracy: 0.9673 - dire_fc_accuracy: 0.9596\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 0.3026 - cate_fc_loss: 0.0321 - dist_fc_loss: 0.1156 - dire_fc_loss: 0.1549 - cate_fc_accuracy: 0.9894 - dist_fc_accuracy: 0.9605 - dire_fc_accuracy: 0.9403\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 1s 17ms/step - loss: 0.3570 - cate_fc_loss: 0.0489 - dist_fc_loss: 0.1683 - dire_fc_loss: 0.1398 - cate_fc_accuracy: 0.9827 - dist_fc_accuracy: 0.9442 - dire_fc_accuracy: 0.9519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2778a43ac40>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import layers, Model, models\n",
    "import joblib\n",
    "\n",
    "def preprocess_and_load_data(csv_path, audio_folder):\n",
    "    # Load CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Initialize LabelEncoders\n",
    "    cate_encoder = LabelEncoder()\n",
    "    dist_encoder = LabelEncoder()\n",
    "    dire_encoder = LabelEncoder()\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Fit the LabelEncoders\n",
    "    cate_encoder.fit(df['cate'])\n",
    "    dist_encoder.fit(df['dist'])\n",
    "    dire_encoder.fit(df['dire'])\n",
    "    \n",
    "    joblib.dump(cate_encoder, 'cate_encoder.pk2')\n",
    "    joblib.dump(dist_encoder, 'dist_encoder.pk2')\n",
    "    joblib.dump(dire_encoder, 'dire_encoder.pk2')\n",
    "    \n",
    "    # Create a list to hold the data\n",
    "    mel_spectrograms = []\n",
    "    labels = []\n",
    "    \n",
    "    # Process each row in the dataframe\n",
    "    for _, row in df.iterrows():\n",
    "        audio_file = os.path.join(audio_folder, row['name'])\n",
    "        y, sr = librosa.load(audio_file, sr=3000, mono=False)  # Load stereo audio at 3 kHz\n",
    "\n",
    "        # Ensure waveform has two channels and transpose to shape (num_samples, 2)\n",
    "        if y.ndim == 1:\n",
    "            y = np.stack([y, y], axis=-1)  # Duplicate if mono\n",
    "        else:\n",
    "            y = y.T  # Transpose to get shape (num_samples, 2)\n",
    "        \n",
    "        # Compute Mel spectrograms for each channel\n",
    "        mel_spectrograms_channel_0 = librosa.feature.melspectrogram(y=y[:, 0], sr=sr, n_fft=2048, hop_length=512, n_mels=128)\n",
    "        mel_spectrograms_channel_1 = librosa.feature.melspectrogram(y=y[:, 1], sr=sr, n_fft=2048, hop_length=512, n_mels=128)\n",
    "        \n",
    "        # Stack the spectrograms to get shape (128, num_frames, 2)\n",
    "        mel_spectrogram = np.stack([mel_spectrograms_channel_0, mel_spectrograms_channel_1], axis=-1)\n",
    "        \n",
    "        # Collect the features and labels\n",
    "        mel_spectrograms.append(mel_spectrogram)\n",
    "        labels.append((\n",
    "            cate_encoder.transform([row['cate']])[0],\n",
    "            dist_encoder.transform([row['dist']])[0],\n",
    "            dire_encoder.transform([row['dire']])[0]\n",
    "        ))\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    mel_spectrograms = np.array(mel_spectrograms)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Create TensorFlow dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((mel_spectrograms, {\n",
    "        'cate_fc': labels[:, 0],\n",
    "        'dist_fc': labels[:, 1],\n",
    "        'dire_fc': labels[:, 2]\n",
    "    }))\n",
    "    \n",
    "    # Set batch size and shuffle\n",
    "    dataset = dataset.shuffle(buffer_size=len(mel_spectrograms))\n",
    "    dataset = dataset.batch(32)\n",
    "    \n",
    "    return dataset, cate_encoder, dist_encoder, dire_encoder\n",
    "\n",
    "# Transformer block\n",
    "def transformer_block(inputs, num_heads, key_dim, ff_dim, dropout_rate=0.1):\n",
    "    # Multi-head attention\n",
    "    attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(inputs, inputs)\n",
    "    attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
    "    out1 = layers.LayerNormalization(epsilon=1e-6)(attn_output + inputs)  # Skip connection\n",
    "\n",
    "    # Feed-forward network\n",
    "    ffn = layers.Dense(ff_dim, activation='relu')(out1)\n",
    "    ffn_output = layers.Dense(inputs.shape[-1])(ffn)\n",
    "    ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n",
    "    return layers.LayerNormalization(epsilon=1e-6)(ffn_output + out1)  # Skip connection\n",
    "\n",
    "# CNN + Transformer model for Mel spectrogram classification\n",
    "def cnn_transformer_model(input_shape=(128, None, 2), num_classes_cate=38, num_classes_dist=7, num_classes_dire=6):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # CNN block for 2D input\n",
    "    x = layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    # Reshape to fit Transformer block\n",
    "    x = layers.Reshape((-1, x.shape[-1]))(x)\n",
    "\n",
    "    # Transformer block\n",
    "    x = transformer_block(x, num_heads=4, key_dim=64, ff_dim=128)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Fully connected layers for each output task\n",
    "    cate_output = layers.Dense(num_classes_cate, activation='softmax', name='cate_fc')(x)\n",
    "    dist_output = layers.Dense(num_classes_dist, activation='softmax', name='dist_fc')(x)\n",
    "    dire_output = layers.Dense(num_classes_dire, activation='softmax', name='dire_fc')(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = models.Model(inputs=inputs, outputs=[cate_output, dist_output, dire_output])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Paths\n",
    "csv_path = 'E:\\\\SIHV2\\\\cleaned\\\\train_split.csv'\n",
    "audio_folder = 'E:\\\\SIHV2\\\\processed_data'\n",
    "\n",
    "# Load dataset\n",
    "dataset, cate_encoder, dist_encoder, dire_encoder = preprocess_and_load_data(csv_path, audio_folder)\n",
    "\n",
    "# Create the CNN+Transformer model\n",
    "model = cnn_transformer_model(input_shape=(128, None, 2), num_classes_cate=4, num_classes_dist=7, num_classes_dire=6)\n",
    "\n",
    "# Compile the model with loss functions and optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss={\n",
    "                  'cate_fc': 'sparse_categorical_crossentropy',\n",
    "                  'dist_fc': 'sparse_categorical_crossentropy',\n",
    "                  'dire_fc': 'sparse_categorical_crossentropy'\n",
    "              },\n",
    "              metrics={\n",
    "                  'cate_fc': 'accuracy',\n",
    "                  'dist_fc': 'accuracy',\n",
    "                  'dire_fc': 'accuracy'\n",
    "              })\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(dataset, epochs=50)\n",
    "\n",
    "# Save the model and weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cnn_transformer_modelv5.h5')\n",
    "model.save_weights('cnn_transformer_weightsv5.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
